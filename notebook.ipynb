{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759b8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import stempeg\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d46c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83961a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(WaveNetBlock, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.conv_filter = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding='same')\n",
    "        self.conv_gate = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding='same')\n",
    "        self.conv_res = nn.Conv1d(out_channels, in_channels, 1)  # Residual connection\n",
    "        self.conv_skip = nn.Conv1d(out_channels, out_channels, 1)  # Skip connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply dilated convolutions\n",
    "        filter_output = torch.tanh(self.conv_filter(x))\n",
    "        gate_output = torch.sigmoid(self.conv_gate(x))\n",
    "        gated_output = filter_output * gate_output\n",
    "        \n",
    "        # Residual and skip connections\n",
    "        residual = self.conv_res(gated_output)\n",
    "        skip_connection = self.conv_skip(gated_output)\n",
    "        output = x + residual\n",
    "        return output, skip_connection\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, num_blocks, num_layers, in_channels, out_channels, residual_channels, skip_channels, kernel_size):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.start_conv = nn.Conv1d(in_channels, residual_channels, kernel_size=1)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            WaveNetBlock(residual_channels, residual_channels, kernel_size, 2 ** i)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.end_conv1 = nn.Conv1d(residual_channels, skip_channels, kernel_size=3, padding=1)\n",
    "        self.end_conv2 = nn.Conv1d(skip_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.start_conv(x)\n",
    "        skip_connections = []\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            for layer in self.blocks:\n",
    "                x, skip = layer(x)\n",
    "                skip_connections.append(skip)\n",
    "\n",
    "        output = torch.relu(sum(skip_connections))\n",
    "        output = self.end_conv1(output)\n",
    "        output = torch.relu(output)\n",
    "        output = self.end_conv2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47911ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnergyConservingLoss(input_mix, input_voice, input_noise, generated_voice):\n",
    "    \n",
    "    voice_L = nn.L1Loss()\n",
    "    noise_L = nn.L1Loss()\n",
    "    \n",
    "    voice_diff = 2 * voice_L(generated_voice, input_voice)\n",
    "    noise_diff = noise_L((input_mix - generated_voice), input_noise)\n",
    "    \n",
    "    loss = voice_diff + noise_diff\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54e8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_musdb(subset):\n",
    "    \n",
    "    assert subset in ['train', 'test']\n",
    "    \n",
    "    mix = []\n",
    "    noise = []\n",
    "    vocals = []\n",
    "\n",
    "    for filename in os.listdir('musdb18/{}'.format(subset)):\n",
    "        \n",
    "        if filename == '.ipynb_checkpoints':\n",
    "            continue\n",
    "            \n",
    "        # Pull training sample from sparser/quieter region\n",
    "        audio, sample_rate = stempeg.read_stems('musdb18/{}/'.format(subset) + filename, \n",
    "                                                stem_id=[0, 3, 4],\n",
    "                                                out_type=np.float32,\n",
    "                                                start=35,\n",
    "                                                duration=5)\n",
    "\n",
    "        mix.append((audio[0].T, sample_rate))\n",
    "        noise.append((audio[1].T, sample_rate))\n",
    "        vocals.append((audio[2].T, sample_rate))\n",
    "        \n",
    "        # Pull training sample from more populated/louder region\n",
    "        audio, sample_rate = stempeg.read_stems('musdb18/{}/'.format(subset) + filename, \n",
    "                                                stem_id=[0, 3, 4],\n",
    "                                                out_type=np.float32,\n",
    "                                                start=60,\n",
    "                                                duration=5)\n",
    "\n",
    "        mix.append((audio[0].T, sample_rate))\n",
    "        noise.append((audio[1].T, sample_rate))\n",
    "        vocals.append((audio[2].T, sample_rate))\n",
    "                \n",
    "    return mix, noise, vocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47490b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(num_blocks=3, num_layers=10, in_channels=2, out_channels=2,\n",
    "                residual_channels=64, skip_channels=256, kernel_size=3)\n",
    "model.to(device) \n",
    "\n",
    "train_mix, train_noise, train_vocals = process_musdb('train')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(len(train_mix)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_mix = torch.tensor(train_mix[i][0]).unsqueeze(0).to(device=device)\n",
    "        input_voice = torch.tensor(train_vocals[i][0]).unsqueeze(0).to(device=device)\n",
    "        input_noise = torch.tensor(train_noise[i][0]).unsqueeze(0).to(device=device)\n",
    "        \n",
    "            \n",
    "        output = model(input_mix)\n",
    "\n",
    "        loss = criterion(output, input_voice) #EnergyConservingLoss(input_mix, input_voice, input_noise, output)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / len(train_mix)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2696bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(train_mix[0][0], rate=train_mix[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated = model(torch.tensor(train_mix[0][0]).unsqueeze(0).to(device=device))\n",
    "Audio(isolated.squeeze(0).cpu().detach().numpy(), rate=train_mix[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
