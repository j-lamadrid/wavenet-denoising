{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb7932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavenet_model import WaveNetModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import stempeg\n",
    "import numpy as np\n",
    "\n",
    "from auraloss.time import SNRLoss, SISDRLoss, SDSDRLoss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f0a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f3d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_musdb(subset):\n",
    "    \n",
    "    assert subset in ['train', 'test']\n",
    "    \n",
    "    mix = []\n",
    "    noise = []\n",
    "    vocals = []\n",
    "\n",
    "    for filename in os.listdir('musdb18/{}'.format(subset))[:1]:\n",
    "        \n",
    "        if filename == '.ipynb_checkpoints':\n",
    "            continue\n",
    "            \n",
    "        # Pull training sample from sparser/quieter region\n",
    "        audio, sample_rate = stempeg.read_stems('musdb18/{}/'.format(subset) + filename,\n",
    "                                                out_type=np.float32,\n",
    "                                                start=30,\n",
    "                                                duration=10)\n",
    "\n",
    "        mix.append((audio[0].T, sample_rate))\n",
    "        noise.append((audio[1].T+audio[2].T+audio[3].T, sample_rate))\n",
    "        vocals.append((audio[4].T, sample_rate))\n",
    "        \n",
    "        # Pull training sample from more populated/louder region\n",
    "        audio, sample_rate = stempeg.read_stems('musdb18/{}/'.format(subset) + filename, \n",
    "                                                out_type=np.float32,\n",
    "                                                start=60,\n",
    "                                                duration=10)\n",
    "\n",
    "        mix.append((audio[0].T, sample_rate))\n",
    "        noise.append((audio[1].T+audio[2].T+audio[3].T, sample_rate))\n",
    "        vocals.append((audio[4].T, sample_rate))\n",
    "        \n",
    "\n",
    "        audio, sample_rate = stempeg.read_stems('musdb18/{}/'.format(subset) + filename, \n",
    "                                                out_type=np.float32,\n",
    "                                                start=45,\n",
    "                                                duration=10)\n",
    "\n",
    "        mix.append((audio[0].T, sample_rate))\n",
    "        noise.append((audio[1].T+audio[2].T+audio[3].T, sample_rate))\n",
    "        vocals.append((audio[4].T, sample_rate))\n",
    "        \n",
    "    mix_out = []\n",
    "    noise_out = []\n",
    "    vocals_out = []\n",
    "\n",
    "    for i in range(len(vocals)):\n",
    "        if np.mean(abs(vocals[i][0][0]) + abs(vocals[i][0][1])) >= 0.05:\n",
    "            mix_out.append(mix[i])\n",
    "            noise_out.append(noise[i])\n",
    "            vocals_out.append(vocals[i])\n",
    "\n",
    "    return mix_out, noise_out, vocals_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7277dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (assuming you have a function process_musdb that loads your data)\n",
    "train_mix, train_noise, train_vocals = process_musdb('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e4d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNetModel(layers=2,\n",
    "                     blocks=2,\n",
    "                     dilation_channels=128,\n",
    "                     residual_channels=128,\n",
    "                     skip_channels=512,\n",
    "                     end_channels=512,\n",
    "                     classes=2,\n",
    "                     output_length=0,\n",
    "                     kernel_size=2,\n",
    "                     dtype=torch.FloatTensor,\n",
    "                     bias=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Avg Loss: 0.2426\n",
      "Epoch [2/50], Avg Loss: 1.1063\n"
     ]
    }
   ],
   "source": [
    "# Define your loss function\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(len(train_mix)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_mix = torch.tensor(train_mix[i][0]).unsqueeze(0).to(device=device)\n",
    "        input_voice = torch.tensor(train_vocals[i][0]).unsqueeze(0).to(device=device)\n",
    "        input_noise = torch.tensor(train_noise[i][0]).unsqueeze(0).to(device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_mix)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion((input_mix[:, :, :440996] - output), input_voice[:, :, :440996])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "            \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "            \n",
    "    avg_loss = total_loss / len(train_mix)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd78820",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(train_mix[0][0], rate=train_mix[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated = model(torch.tensor(train_mix[0][0]).unsqueeze(0).to(device=device))\n",
    "Audio(torch.tensor(train_mix[0][0])[:, :440996].cpu().detach().numpy() - isolated[0].squeeze(0).cpu().detach().numpy(), rate=train_mix[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b932f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
